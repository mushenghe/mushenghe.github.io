<!DOCTYPE html>
<html lang="en">

  <head>
    <title> Musheng He</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <!--<title>Agency - Start Bootstrap Theme</title>-->

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Musheng He</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">           
          <div class="intro-heading text-uppercase"style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Musheng He</div>
          <div class="intro-lead-in">Robotics Systems Helping People Solve Real Life Challenges</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/1exi0S5WqNtpWWaF2SKWeDWSirYh6uDBI/view?usp=sharing">Resume</a>
        </div>
      </div>
    </header>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <h3 class="section-subheading text-muted">September 2019 ~ Now</h3>
          </div>
        </div>
        <div class="row">

         <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#musclesynergy">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/muscle_proj/task.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Muscle Synergy Identification</h4>
              <p class="text-muted">Unsupervised Learning | Biomedical Science<br>Using Nonnegative Matrix Factorization to Identify Muscle Synergies During a Multi-Joint Task</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#healerbaxter">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/healer_baxter/healer.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Healer Baxter</h4>
              <p class="text-muted">Deep Learning | CV | Motion Planning<br>Baxter Robot play the piano based on detected human emotion</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#terminator">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/mind_proj/nerfgun.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Terminator</h4>
              <p class="text-muted">ROS | CV | Motion Planning<br>Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose</p>
            </div>
        </div>

            <div class="col-md-4 col-sm-6 portfolio-item">
                <a class="portfolio-link" data-toggle="modal" href="#kuka">
                  <div class="portfolio-hover">
                    <div class="portfolio-hover-content">
                      <i class="fas fa-plus fa-3x"></i>
                    </div>
                  </div>
                  <img class="img-fluid" src="img/portfolio/manipulation_proj/manipulation.gif" alt="" width="400" height="300">
                </a>
                <div class="portfolio-caption">
                  <h4>Mobile Manipulation</h4>
                  <p class="text-muted">Manipulation | Motion Planning<br>KUKA youBot to pick up a block at the start location and carry it to the desired location in the simulation software V-REP</p>
                </div>
              </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#pushinggrasping">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/pushing_grasping/rep.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Visual Pushing and Grasping</h4>
              <p class="text-muted">Deep Reinforcement Learning | Docker <br>Train robotic agents to learn to plan pushing and grasping actions for manipulation with deep reinforcement learning</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#panorama">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/image_mosaic/panorama.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>ImageMosaic</h4>
              <p class="text-muted">Computer Vision<br>Create an image panorama by stitching a set
                of images together</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#dynamics">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/dynamics_proj/314project.gif" alt="" width="400" height="150">
            </a>
            <div class="portfolio-caption">
              <h4>Pin-triangle Dynamics Simulation </h4>
              <p class="text-muted">Dynamics | Simulation<br>Simulation of a triangle bouncing in the enclosed rectangle</p>
            </div>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Technical Skills -->
    <section  id="TechnicalSkills">
        <div class="container">
          <div class="row">
            <div class="col-lg-12 text-center">
              <h2 class="section-heading text-uppercase">Skills</h2>
              <p></p>
            </div>
          </div>
          <div class="row text-center">
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/microcontroller.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Robotics</h4>
                <p class="text-muted"> ROS,Linux,Git,Gazebo <br> Computer Vision, Machine Learning<br> Motion Planning,Feedback Control,Cloud Computing</p>
            </div>
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/coding.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Programming</h4>
                <p class="text-muted">C++, Cï¼ŒPython, Java, MATLAB, PLC, VHDL</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <!-- <i class="fas fa-circle fa-stack-2x text-primary"></i> -->
                <img class="mx-auto rounded-circle" src="img/skills/electronicdesign.png" alt="" width="120" height="120">
              </span>
              <h4 class="service-heading">Electrical Engineering</h4>
              <p class="text-muted">Power System, Microprocessor<br>PCB Design, Electronic System Design</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/manufacturing.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Manufacturing</h4>
              <p class="text-muted"> Laser Cutter, 3D print, soldering </p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/cad.png" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Mechanical Engineering</h4>
              <p class="text-muted">Onshape, AutoCAD<br> Mechatronics/ Microcontroller</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/machine_learning.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Research Interest</h4>
              <p class="text-muted">Assistive Robotics, Rehabilation<br>Human Movement Science</p>
            </div>
          </div>
        </div>
      </section>
    
    <!-- contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
            <div class="col-lg-12 max-auto mb-5">
                <img class="mx-auto d-block rounded-circle" src="img/profilepic.jpg" alt="Avatar" width="300" height="300">
            </div>
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Contact me</h2>
           
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
         

            <div class="team-member">
              
              <h4>Musheng He</h4>
              <p class="text-muted">Master of Science in Robotics @ Northwestern University</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/mushenghe">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:mushenghe2020@u.northwestern.edu">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/musheng-he-a50277176/">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-8 mx-auto text-center">
           
          </div>
        </div>
      </div>
    </section>


    <!-- Portfolio Modals -->

    <!-- Muscle Synergy Identification -->
    <div class="portfolio-modal modal fade" id="musclesynergy" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Muscle Synergy Identification</h3>
                    <a class="boxed">Unsupervised Learning</a>&ensp;<a class="boxed">Signal Processing</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Matlab</a>&ensp;
                    <p></p><br>
                    <h4>Brief Description</h4>
                    <p></p>     
                    <p>This project aims to use nonnegative matrix factorization algorithm to identify muscle synergies during a multi-joint task</p>
                    <h4>Motivation</h4>                   
                    <p>When doing multi-joint task involving lifting up the shoulder and bending the elbow, what really happened is that there are some movement-related signals coming from our brain,
                        in specific coming from our cerebral cortex, and reaching the motor neuron at the spinal cord and traveling all way down through the axons and synapsing
                        with the corresponding muscle fibers and finally creating the torque out. Instead of having to control dozens of muscles individually, the signals coming from our brain just control the activation
                        of a much smaller number of muscle groups. This muscle synergies is part of a hierarchical control strategy. Thes synergies identify the relevant muscle groups that when activated together, allow for simplified control of particular biomechanical features of the limb.</p>
                    <p></p>
                    <h4>Experiment Setup</h4>
                    <P></P>
                    <h5>System Pipeline</h5>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/muscle_proj/pipeline.png" alt="">
                    <P></P>
                    <p>The system acquires torque data through the torque sensor as well as the EMG signal through 8 surface EMG electrodes. The torque data indicates the extent to which the participant is flexing about the testing elbow joint. 
                        The EMG signal indicates the electical activity within each of the 8 testing muscles. We used a DAQ card for data acquisition. A Matlab program streams the sensory data and convert the voltage signal
                        to the actual torque generated by the participant. A python program processes the collected data and use matrix factorization algorithm to find the synergies behind the EMG data  </p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/muscle_proj/EMG.png" alt="">
                    <h5>Participants and Conditions</h5>
                    <P></P>
                    <h6>Participants</h6>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/participants.png" alt="" width="2000" height="1000">
                    <P>One male and three female participants with no hemiparetic stroke were tested. They were all right-hand dominant, and with a mean Â± standard deviation age of 26 Â± 3 (range: 22-29) </P>
                    <h6>Conditions</h6>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/conditions.png" alt="" width="400" height="300">
                    <p>The participant was asked to pull in using 2 different shoulder abduction loads (10% and 50% of the maximum shoulder abduction torque)</p>
                    <h5>Experiment Procedure</h5>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/setup.png" alt="" width="2000" height="1000">
                    <p>The participant was requested to not exercise the day before and of testing to avoid muscle fatigue. At the beginning of the testing session, the participant sat with their torso and waist strapped to the Biodex chair. 
                        The participant's testing arm was affixed to an isometric measurement device at 85Â° shoulder abduction, 40Â° shoulder flexion, and 90Â° elbow flexion.</p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/emg-setup.png" alt="" width="1000" height="500">
                    <p>8 electrode were placed on the surface of the personâ€™s skin in order to record the muscle activities.</p>
                    <h6>Demo</h6>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/x7HlRIp5OJ0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p></p>
                    <h4>Data Analysis</h4>
                    <P></P>
                    <h5>Muscle Synergy</h5>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/intuition.png" alt="" width="1000" height="500">
                    <p>In order to condense the information and use less amount of muscle groups to explain all of the emgs, the idea of motor synergy was taken to find the muscle synergies. The goal here is to find the coordinated recruitment of groups of muscles with specific activation balances</p>
                    <h5>Hypothesis</h5>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/hypothesis.png" alt="" width="1000" height="500">
                    <p>The anticipated result is that there should be one synergy reinforces the elbow flexing and one reinforces the shoulder abduction.
                        Since bicep is the main muscle for elbow flexing and the anterior deltoid is the main muscle for shoulder abduction, the hypothesis is made that the bicep is the 
                    main muscle for one synergy group and the anterior deltoid is the main muscle for another synergy group </p>
                    <p></p>
                    <h5>Methods</h5>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/intuition-algo.png" alt="" width="1000" height="500">
                    The unsupervised learning method â€”â€” Nonnegative matrix factorization algorithm (NMF) is taken to find the muscle synergies that explains the 8 muscle activations. The squared residual and VAF measurements are taken to estimate the
                    minimum number of muscle synergies, and cross-validation approach is used to help deriving a more accurate estimation.
                    <p></p>
                    <h6>EMG Preprocessing</h6>
                    <p>EMGs were processed to remove baseline voltage drift, rectified, 
                        and then averaged over the 0.5-s target matching interval. Mean baseline EMGs (recorded while subjects
                        sat with their arm affixed to an isometric device without force generation) were subtracted from the averaged data. As a result, EMG data for each trial were vectors whose dimension was 8 (the number of
                        muscles recorded) and these data reflected the increase in muscle activity corresponding to active force
                        production. Before synergy extraction, EMG data recorded from each muscle were concatenated across
                        trials relevant to the purpose of the synergy extraction and normalized by dividing each of the EMG channel by its maximum to have unit variance. This
                        normalization procedure ensured that subsequent synergy extraction from preprocessed EMGs was not
                        biased towards high-variance muscles.</p>
                    <p></p>
                    <h6>Identification of Muscle Synergies â€”â€” NMF Algorithm</h6>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/NMF.png" alt="" width="1000" height="500">
                    <p></p>
                    <h6>Estimate the number of muscle synergies (k)</h6>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/vaf.png" alt="" width="1000" height="500">
                    <h6>Algorithm</h6>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/algo.png" alt="" width="1000" height="500">
                    <p></p>
                    <h4>Results</h4>
                    <p></p>
                    <h5>MSE Box Plot</h5>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/mse.png" alt="" width="500" height="400">
                    <p>The training error and testing error were plotted out based on the collected data. The box plot shows a clear trend in training error, where the mean square error of the training set decreases
                    as the number of muscle synergy increases. But there isn't the same trend in the testing set, since the mean and standard deviation of the testing error for each number of muscle synergy doesn't have much difference among each other. Thus, the mean square error is not sufficient
                    to estimate the minimum number of muscle synergies. Thus the VAF measurement is used to help estimate the number of muscle synergies <p>
                    <p></p>
                    <h5>MSE and VAF measurement results </h5>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/VAF.png" alt="" width="700" height="300">
                    <p></p>
                    <p>According to the requirement that adding another synergy increased the mean global VAF < 3%, the number of muscle synergy for the 10% shoulder abduction load task was chosen to be 4 and that for the 50% shoulder abduction load task was chosen to be 2</p>
                    <h5>Normalized Muscle synergy when shoulder abduction load is 10% (# synergies = 4)</h5>
                    <ul class="list-inline">
                    <p class="item-intro text-muted"> <a class="boxed" href="https://github.com/mushenghe/muscle_synergy_torque_accuracy"><font color="red"><b>Current Status and Source code here</b></font></a></p>
                      <li>Date: December 2019</li>
                     
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    <!-- Healer Baxter -->
    <div class="portfolio-modal modal fade" id="healerbaxter" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Healer Baxter</h3>
                    <a class="boxed">Deep Learning</a>&ensp;<a class="boxed">Tensorflow</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                    <p></p>
                    <h4>Brief Description</h4>       
                  	<p>This project aims to use deep learning and manipulation platform to train a two-armed robot to play the piano based on the human emotion.</p>
                  	<p></p>
                    <h4>Motivation</h4>       
                  	<p>We live in an era in which communication seems simpler than any times, a friend is only one text away or one video chat away. Although communication may be easier and faster, people still feel lonely and depression rates have largely increased. Inspired by this circumstance, this project is to design a system that would enable the Baxter Robot to detect the negative emotion of its master and play songs on the piano using both hands to help him/her get rid of the bad feelings.</p>
                    <p></p>
                    <h4>Emotion Detection</h4>
                    <h5>1. Data Pre-processing</h5> 
                    <p>The datasets are composed of 35887
                      training and testing samples. The training
                      samples are then divided into two sets namely;
                      Training Set and Validation Set. Training set
                      samples composed of 80% of the original dataset
                      samples and 20% of the samples are specified for
                      validation. Hence Training Set and Validation Set
                      will be 22966 and 5741 respectively.
                      Preprocessing is performed to prepare images for
                      the feature extraction stage. A set of facial feature
                      points is extracted from the images then facial
                      features derived from these points. Different sets
                      of facial features are used for both training and
                      validation classifiers.</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/data.png" alt="">
                    <p></p>
                    <h5>2. Data Augmentationg</h5> 
                    <p>In order to avoid overfitting and improve
                      recognition accuracy I applied data
                      augmentation techniques on each training samples. For each image I performed
                      following transforms:
                      a. Rescale (1. / 255)
                      B. Rotation (30)
                      b. Shear (0.3)
                      c. Zoom (0.3)
                      d. shift(width:0.4,height:0.4)
                      e. Flip (horizontal)</p>
                    <p></p>
                    <h5>3. Proposed CNN Model</h5>       
                  	<p>The CNN Model I built takes the input grayscale image size of 48*48 pixels. This model architecture is composed of 5 layers. These layers
                      contains 5 convolutional layers and 5 max pooling layers along with 2 fully connected layer and the output layer. The
                      output layer consists of 5 neurons corresponding to 5
                      emotional labels: Angry, Happy, Neutral, Sad and Surprise. This model uses Rectified Linear Unit (relu) as most precisely used
                      activation function which is applied on all the Convolution
                      Layer and Dense Layer except the last layer (output layer)
                      which is actually Softmax Function. Dropout Layer is also
                      applied after each Convolution, Max Pooling and Dense
                      Layer with the rate of 0.25</p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/CNN_ARC.png" alt="">
                    <p></p>
                  	<h5>Emotion Detection Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/s_xAZkpSfBo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <h4>Piano Playing</h4>
                    <p>The detected emotion would be written in a text file using basic pyhton I/O function. The piano playing node keep reading this text file and when the emotion "sad" is detected and written in the file, baxter would start playing the piano</p>
                    <h5>Locate keys using AprilTags</h5> 
                    <p>Use pose infomation of the two apriltags in left_arm frame to compute the poses of keys in the baxter base frame</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/piano.png" alt="">
                    <h5>Piano Playing Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/lJ78-UlrxD4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/mushenghe/HealerBaxter"><font color="red"><b>Github Page</b> </font></a></p>
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                      
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  
    <!-- Terminator -->
    <div class="portfolio-modal modal fade" id="terminator" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Terminator</h3>
                  <a class="boxed">ROS</a>&ensp;<a class="boxed">Baxter</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                  <p></p>         
                  <h4>Brief Description</h4>       
                  <p>The goal of this project is to enable Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose. </p>
                  <h4>Action Sequence</h4> 
                  <p></p>
                  <p>1. Baxter goes through initial calibration and start up sequence. Arms are moved to an initial pose. <br>
                  2. Baxter finds the nerf gun using an AprilTag and its left arm camera<br>
                  3. Baxter moves its left arm to line up with the nerf gun and closes its gripper to pick up the gun<br>
                  4. Once Baxter has the gun, it uses its left arm camera to find a cup using darknet<br>
                  5. Baxter keeps moving its left arm until the cup is in the center of the image produced by the camera<br>
                  6. Baxter moves its right arm to put its grippers around the nerf gun trigger<br>
                  7. Baxter waits for a user input to confirm the firing of the gun<br>
                  8. Baxter keeps waiting until the user tells it to fire<br>
                  9. Baxter pulls the trigger using its right gripper<br>
                  10. Baxter moves to a final pose right after shooting<br></p>
                  <p></p>
                  <h4>Demo</h4>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/2MRsNefNWmw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p></p>

                  <ul class="list-inline">
                  <p><a class="boxed" href="https://github.com/mushenghe/final-project-terminator"><font color="red"><b>Github Page</font></a></p> </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <!-- KUKA Manipulation -->
    <div class="portfolio-modal modal fade" id="kuka" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Mobile Manipulation</h3>
                  <a class="boxed">Manipulation</a>&ensp;<a class="boxed">Motion Planning</a>&ensp;<a class="boxed">VREP</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to drive the KUKA youBot to pick up a block at the start location, carry it to the desired location, and put it down in the simulation software V-REP. The project covers the following topics: <br>1. Plan a trajectory for the end-effector of the youBot mobile manipulator. <br>2. Generate the kinematics model of the youBot, consisting of the mobile base with 4 mecanum wheels and the robot arm with 5 joints<br>3. Apply feedback control to drive the robot to implement the desired task<br>4. Conduct the simulations in V-REP</p>                           
                  <h5>First Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/C3QQO7TZn4g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <h5>Second Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/1WKscbUi3HA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p><a class="boxed" href="https://github.com/mushenghe/Mobile-Manipulation-"><font color="red"><b>Github Page</font></a></p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Visual Pushing and Grasping -->
    <div class="portfolio-modal modal fade" id="pushinggrasping" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                <div class="rl"></div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Visual Pushing and Grasping</h3>
                    <a class="boxed">Deep Reinforcement Learning</a>&ensp;<a class="boxed">Docker</a>&ensp;<a class="boxed">Pytorch</a>
                    <p></p>
                    <h4>Brief Description</h4> 
                    <p>Most grasping algorithms today often fail to handle scenarios where objects are tightly packed together. They can attempt bad grasps repeatedly to no avail since they can only find accessible grasps. This project proposed to discover and learn synergies between pushing and grasping from experience through model-free deep reinforcement learning.</p>
                    <h5>System Overview</h5> 
                    <p></p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/pushing_grasping/method.jpg" alt="">
                    <p></p>
                    <h5>Model Input & Output</h5>
                    <p></p>
                    <p>The Q-function is modeled as two feed-forward fully convolutional networks(FCNs) Î¦p and Î¦g. FCN Î¦p is for pushing motion primitive behavior and FCN Î¦g is for grasping.</p>
                    <p></p>
                    <p>For each individual FCN Î¦Ïˆ:</p>
                    <p></p>
                    <p>Input: the heightmap image representation of the current state</p>
                    <p>Output: a dense pixel-wisemap of Q values with the same image size and resolution as that of the state</p>
                    <p></p>
                    <p>Note: each individual Q value prediction at a pixel p represents the future expected reward of executing primitive Ïˆ at 3D location q where qâ†’p âˆˆst.</p>  
                    
                    <p></p>
                    <h5>Use the pretrained model</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/i_fHtQ-zDkM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/mushenghe/visual-pushing-grasping"><font color="red"><b>Github Page</font></a></p> 
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                        <i class="fas fa-times"></i>
                        Close Project</button>
                    </div>
                </div>
                </div>
            </div>
            </div>
        </div>
        </div>
      

    <!-- Panorama -->
    <div class="portfolio-modal modal fade" id="panorama" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">ImageMosaic</h3>
                  <a class="boxed">Computer Vision</a>&ensp;<a class="boxed">MATLAB</a>&ensp;<a class="boxed">Python</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to Create an image panorama by stitching a set of images together</p>                           
                  <h5>Image Registration</h5>
                  <p>I used SURF to do the feature point extraction and matching, then used random sample consensus(RANSAC) for transform matrix estimation</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/SURFMatches.jpg" alt="">
                  <h5>Image Warping</h5>
                  <p>Use the derived transform matrix nad project that warped image on a plain surface</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img2.jpg" alt="">
                  <h5>Image Blending</h5>
                  <p>Using Center-Weighting algorithm (compute the the distance from each pixel to 4 boundaries of the image and take the the smallest ratio                   
                    between two distances and the dimension of image as the corresponding pixel
                    value on mask matrix). The mask we derived is shown in the following image:</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/mask.jpg" alt="">
                  <p>For each image, I derive a mask and then warp the mask just as warp the image</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/before.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/after.jpg" alt="">
                  <h5>Cropping</h5>
                  <p>After doing image stitching and image blending, I get the panorama look as following</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3.jpg" alt="">
                  <p>Use pythong to find the largest rectangle that donâ€™t include the black region in the
                    panorama image, I get the final panorama look as following</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3_a.jpg" alt="">
                  <ul class="list-inline">
                  <p><a class="boxed" href="https://drive.google.com/file/d/1p8h66kcTu3J3DAQ-OETgzaHgj101Kw8o/view?usp=sharing"><font color="red"><b>Report</font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Dynamics -->
    <div class="portfolio-modal modal fade" id="dynamics" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Pin-triangle Dynamics Simulation </h3>
                  <a class="boxed">Python</a>&ensp;<a class="boxed">Dynamics</a>&ensp;<a class="boxed">Simulation</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>This project is a dynamics simulation of a triangle bouncing in the enclosed rectangle. This project shows techniques expansion from theory and ability of building physical model.  </p>
                  <p></p>
                  <h5>dynamic model</h5>
                  <p>The pictured pin-triangle is a constrained system involving 2 bodies: a equilateral triangle and a square. the triangle has length d = 3, mass m =0.5 and rotational inertia J=1 (assuming that the center of mass is at the center of geometry). The square has length D = 20, mass M = 5 and rotational inertia J=3 (assuming that the center of mass is at the center of geometry). The triangle has configuration (x,y,\theta_t) and the square has an angle \theta_s relative to the world frame. The triangle is constrainted to not bounce out of the square.</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/dynamics_proj/model.png" alt="">
                  <ul class="list-inline">
                 	<p><a class="boxed" href="https://colab.research.google.com/drive/1d3GrjH75j-MqSYNUyhwd3FWeb2eVgY4D"><font color="red"><b>Python Source Code</font></a></p>
                    
                   
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                    
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

   

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
