<!DOCTYPE html>
<html lang="en">

  <head>
    <title> Musheng He</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <!--<title>Agency - Start Bootstrap Theme</title>-->

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Musheng He</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">           
          <div class="intro-heading text-uppercase"style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Musheng He</div>
          <div class="intro-lead-in">Robotics Systems Helping People Solve Real Life Challenges</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/1exi0S5WqNtpWWaF2SKWeDWSirYh6uDBI/view?usp=sharing">Resume</a>
        </div>
      </div>
    </header>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <h3 class="section-subheading text-muted">September 2019 ~ Now</h3>
          </div>
        </div>
        <div class="row">

         <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#musclesynergy">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/muscle_proj/task.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Muscle Synergy Identification</h4>
              <p class="text-muted">Unsupervised Learning | Biomedical Science<br>Using Nonnegative Matrix Factorization to Identify Muscle Synergies During a Multi-Joint Task</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#healerbaxter">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/healer_baxter/healer.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Healer Baxter</h4>
              <p class="text-muted">Deep Learning | CV | Motion Planning<br>Baxter Robot play the piano based on detected human emotion</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#terminator">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/mind_proj/nerfgun.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Terminator</h4>
              <p class="text-muted">ROS | CV | Motion Planning<br>Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose</p>
            </div>
        </div>

            <div class="col-md-4 col-sm-6 portfolio-item">
                <a class="portfolio-link" data-toggle="modal" href="#kuka">
                  <div class="portfolio-hover">
                    <div class="portfolio-hover-content">
                      <i class="fas fa-plus fa-3x"></i>
                    </div>
                  </div>
                  <img class="img-fluid" src="img/portfolio/manipulation_proj/manipulation.gif" alt="" width="400" height="300">
                </a>
                <div class="portfolio-caption">
                  <h4>Mobile Manipulation</h4>
                  <p class="text-muted">Manipulation | Motion Planning<br>KUKA youBot to pick up a block at the start location and carry it to the desired location in the simulation software V-REP</p>
                </div>
              </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#pushinggrasping">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/pushing_grasping/rep.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Visual Pushing and Grasping</h4>
              <p class="text-muted">Deep Reinforcement Learning | Docker <br>Train robotic agents to learn to plan pushing and grasping actions for manipulation with deep reinforcement learning</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#panorama">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/image_mosaic/panorama.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>ImageMosaic</h4>
              <p class="text-muted">Computer Vision<br>Create an image panorama by stitching a set
                of images together</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#dynamics">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/dynamics_proj/314project.gif" alt="" width="400" height="150">
            </a>
            <div class="portfolio-caption">
              <h4>Pin-triangle Dynamics Simulation </h4>
              <p class="text-muted">Dynamics | Simulation<br>Simulation of a triangle bouncing in the enclosed rectangle</p>
            </div>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Technical Skills -->
    <section  id="TechnicalSkills">
        <div class="container">
          <div class="row">
            <div class="col-lg-12 text-center">
              <h2 class="section-heading text-uppercase">Skills</h2>
              <p></p>
            </div>
          </div>
          <div class="row text-center">
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/microcontroller.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Robotics</h4>
                <p class="text-muted"> ROS,Linux,Git,Gazebo <br> Computer Vision, Machine Learning<br> Motion Planning,Feedback Control,Cloud Computing</p>
            </div>
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/coding.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Programming</h4>
                <p class="text-muted">C++, C，Python, Java, MATLAB, PLC, VHDL</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <!-- <i class="fas fa-circle fa-stack-2x text-primary"></i> -->
                <img class="mx-auto rounded-circle" src="img/skills/electronicdesign.png" alt="" width="120" height="120">
              </span>
              <h4 class="service-heading">Electrical Engineering</h4>
              <p class="text-muted">Power System, Microprocessor<br>PCB Design, Electronic System Design</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/manufacturing.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Manufacturing</h4>
              <p class="text-muted"> Laser Cutter, 3D print, soldering </p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/cad.png" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Mechanical Engineering</h4>
              <p class="text-muted">Onshape, AutoCAD<br> Mechatronics/ Microcontroller</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/machine_learning.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Research Interest</h4>
              <p class="text-muted">Assistive Robotics, Rehabilation<br>Human Movement Science</p>
            </div>
          </div>
        </div>
      </section>
    
    <!-- contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
            <div class="col-lg-12 max-auto mb-5">
                <img class="mx-auto d-block rounded-circle" src="img/profilepic.jpg" alt="Avatar" width="300" height="300">
            </div>
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Contact me</h2>
           
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
         

            <div class="team-member">
              
              <h4>Musheng He</h4>
              <p class="text-muted">Master of Science in Robotics @ Northwestern University</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/mushenghe">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:mushenghe2020@u.northwestern.edu">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/musheng-he-a50277176/">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-8 mx-auto text-center">
           
          </div>
        </div>
      </div>
    </section>


    <!-- Portfolio Modals -->

    <!-- Muscle Synergy Identification -->
    <div class="portfolio-modal modal fade" id="musclesynergy" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Muscle Synergy Identification</h3>
                    <a class="boxed">Unsupervised Learning</a>&ensp;<a class="boxed">Signal Processing</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Matlab</a>&ensp;
                    <p></p><br>
                    <h4>Brief Description</h4>
                    <p></p>     
                    <p>This project aims to identify how muscles are synergistically activated at the arm during a multi-joint task.
                    </p>
                    <h4>Motivation</h4>  
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/retrieve.gif" alt="" width="400" height="250">  
                    <p>Figure 1</p>               
                    <p>While seemingly simple, a motor task, such as retrieving an object from a high shelf, is very complicated. To achieve this task, movement-related signals 
                        are generated from the motor cortex of our brain and relayed to muscle fibers via motor neurons. In turn, muscles contract and the arm is raised and extended. 
                        There are more than 600 muscles in our body.  Instead of controlling each muscle individually, our brain is thought to recruit these muscles in set groups. 
                        The activation of muscles in this grouped manner is termed muscle synergies and is part of a hierarchical control strategy. Activation of muscle synergies, 
                        rather than individual muscles, allows for a simplified control of one’s limb. In this work, the focus is on muscle activation at the arm. Specifically, 
                        the goal is to determine how muscles concurrently activate when an individual abducts at the shoulder and flexes at the elbow. As such, we can determine 
                        normal muscle activation patterns in a population that is neurologically and orthopaedically intact.
                    </p>
                    <p></p>
                    <h4>Hypothesis</h4>
                    <p></p>
                    <p>For a task involving abduction at the shoulder and flexion at the elbow, the main muscles involved are the biceps, triceps lateral, anterior deltoid, medial 
                        deltoid, posterior deltoid, pectoralis major, lower trapezius, and middle trapezius.  Based on the muscle synergy hypothesis, we would expect to observe at 
                        least two synergies for this task: one that is mainly responsible for the elbow flexion and one for shoulder abduction. 
                    </p>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/synergy.png" alt="" width="1000" height="500">
                    <p>Figure 2</p>    
                    <p></p>
                    <h4>Experimental Setup</h4>
                    <P></P>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/muscle_proj/pipeline.png" alt="" width="400" height="300">
                    <p>Figure 3 <sup>[1]</sup></p>    
                    <p></p>
                    <p>The experimental setup, as shown in Figure 3, was comprised of a custom mechatronic system, a monitor, speakers, and Biodex chair. The system acquires torque data from a six-degree-of-freedom load cell. The acquired data, in conjunction with a 
                        biomechanical model, indicate the extent to which the participant flexes about the testing elbow and abducts about the shoulder.  In addition, the system 
                        quantifies muscle activity  using eight surface electromyography (sEMG) electrodes (sEMG1: biceps, sEMG2: triceps lateral, sEMG3: anterior deltoid, sEMG4: 
                        medial deltoid, sEMG5: posterior deltoid, sEMG6: pectoralis major, sEMG7: lower trapezius, and sEMG8: middle trapezius). The sEMG signals indicate the 
                        electrical activity within each of the eight testing muscles. A DAQ card acquires data from these sensors, and a Matlab program streams the data. Data 
                        is collected at 1kHz.
                    </p>                   
                    <P></P>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/muscle_proj/EMG.png" alt="">                   
                    <p></p>
                    <p>Figure 4</p>    
                    <p></p>
                    <h5>Participants</h5>
                    <P></P>
                    <P>Data were collected from one male and three female participants. Participants were all right-hand dominant, and their mean ± standard deviation age was 26 ± 3 (range: 22-29)
                    </P>
                    <h5>Experimental Procedures</h5>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/setup.png" alt="" width="3000" height="1000">
                    <P>Figure 5</P>
                    <p></p>
                    <p>The participant was requested to not exercise the day before and of testing to avoid muscle fatigue. At the beginning of the testing session, the participant 
                        sat with their torso and waist strapped to the Biodex chair. The participant's testing arm was affixed to an isometric measurement device at 85° shoulder 
                        abduction, 40° shoulder flexion, and 90° elbow flexion. 
                    </p>
                    <h6>Baseline Task</h6>
                    <p>This task was completed when the participant was relaxing without generating any torques with their right arm.
                    </p>
                    <p></p>
                    <h6>Maximum Voluntary Torque Generation Task</h6>
                    <p>The participant was asked to maximally activate each of the eight muscles and the maximum sEMG data was obtained for each respective muscle.
                    </p>
                    <p></p>
                    <h6>Isometric Torque Generation Task</h6>
                    <p>The participant was asked to flex about their elbow to 25% of their maximum voluntary torque in elbow flexion. This task was completed when abducting to two different shoulder abduction loads: 10% and 50% of their maximum voluntary torque in shoulder abduction. When the participant achieved desired shoulder abduction and elbow flexion torques, they were asked to maintain the desired torque for two seconds. For each shoulder abduction load, the participant performed 10 trials. During the two-second hold period, we extracted 0.5s of data for subsequent data analyses. 
                    </p>
                    <p></p>
                    <h6>Demo</h6>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/x7HlRIp5OJ0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p></p>

                    <h4>Data Analysis</h4>
                    <P></P>
                    <h5>sEMG Preprocessing</h5>
                    <p></p>
                    <p>Baseline sEMGs were first obtained when the participant was relaxing without generating any torques with their right arm. To remove baseline voltage drift, first the mean baseline sEMG signal for each sEMG channel was identified from the data acquired in the baseline task. Following, this mean baseline sEMG signal was subtracted from the corresponding channel’s sEMG data acquired throughout each isometric torque generation trial. Next, the sEMG signals were rectified and averaged over a 0.5 seconds interval. Then, EMG data were normalized to the maximum to ensure that subsequent synergy extraction from preprocessed EMGs was not biased towards muscles with higher output. EMG data for each trial were stored in eight-column matrices and then were concatenated across the twenty trials.
                    </p>
                    <p></p>
                    <h5>Muscle Synergy Identification</h5>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/algo.png" alt="" width="500" height="300">
                    <p>Figure 6</p>
                    <p>A custom python program identifies the underlying muscle synergies that explain the patterns observed in the eight activated muscles during the 
                        isometric torque generation task. The program was designed to process the collected sEMG data and apply a matrix factorization algorithm to 
                        decompose the EMG signals. The unsupervised learning method we employed was nonnegative matrix factorization(NMF), an approach that has been 
                        previously used in literature (Lee and Seung 1999). 
                        The sEMG data we collected was split into a training set and a testing set by randomly holding out 20% of the data as testing data and leaving 
                        80% as training data.We used mean square error (MSE) to evaluate the error in the training set and testing set during each NMF update iteration. 
                        MSE was calculated as the squared residual: MSE =| sEMG -W·S |&sup2. where sEMG was a 20 (number of trials) by 8 matrix containing the 8 EMGs (of unit magnitude) in each row and W was a 20 by K (number of 
                        synergies) matrix, with each row containing the synergy activation coefficients for a specific trial. S was a K by 8 matrix, with each 
                        row containing the synergy activation strength for each of the 8 muscles.
                    </p>        
                    <p>A goal in our approach was to prevent overfitting the data, as well as ensuring the selection of the minimum number of muscle synergies that can explain the observed muscle activation patterns. Therefore, we included the following procedures. We first evaluated how the derived muscle synergies can explain the observed pattern by using the variance accounted for (VAF). VAF is defined as:
                    VAF = 100 * (1- SSE / SST). where SSE was the sum of the squared residuals and SST was the sum of the squared EMG data. This approach requires that adding another synergy increases the mean global VAF by at least 3% (Jinsook 2013). 
                    </P>
                    <h4>Results</h4>
                    <p></p>
                    <h5>Number of Muscle Synergies Selected </h5>
                    <P></P>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/mse.png" alt="" width="500" height="400">
                    <P>Figure 7</P>
                    <p>The training error and testing error were plotted based on the collected data. As the number of muscle synergy increases, 
                        the mean squared error of the training set decreases. Therefore, the synergies selected seemingly better explained the data 
                        with an increased number of muscle synergies. However, it is possible that the reduced mean square error of the training set 
                        is due to overfitting of the data in the training set. As such, we needed to determine whether the same trends occurred in the
                         testing set.
                    <p>
                    <p>The mean squared error of the testing set error based on the two, three, and four muscle synergies, respectively, that were extracted in the training set,  were similar. As such, this points to the decrease in the mean squared error in the training set arising due to an overfitting of the training set data. This overfitting indicates that MSE is not an effective measure to evaluate the outcome of the NMF algorithm. 
                    </p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/VAF.png" alt="" width="700" height="300">
                    <p>Table 1</p>
                    <p>Previous studies (Jinsook 2012) identifying muscle synergies used VAF to judge the performance of the sEMG signals decomposition. 
                        Therefore, we adopted this approach of using VAF to estimate the number of muscle synergies. Table 1 outlines the VAF obtained for 
                        the two motor tasks tested in this study.  Our requirement was that  the addition of another synergy increases the mean global VAF 
                        by at least 3%. Hence, the number of muscle synergies selected for the 10% shoulder abduction load task was four and for the 50% 
                        shoulder abduction load task was two.
                    </p>
                    <h5>Muscle Activation Patterns </h5>
                    <p></p>
                    <h6>Normalized Muscle synergy when shoulder abduction load is 10% MVT Shoulder abduction
                    </h6>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/10.png" alt="" width="1000" height="700">
                    <p>Figure 8</p>
                    <p></p>
                    <p>There are four muscle synergies decomposed from the sEMG signals collected during the motor task when participants flexed 
                        about their elbow to 25% maximum voluntary elbow flexion and abducted about their shoulder to 10% maximum voluntary shoulder 
                        abduction. As shown in Figure 8, synergy group 2 has strong involvement of the anterior deltoid and middle trapezius muscles 
                        which are responsible for shoulder abduction. Synergy group 4 primarily consists of the triceps brachii muscle, which was not 
                        expected. A rationale for why the triceps were activated is to stabilize the arm by increasing its stiffness. Synergy group 1 
                        has strong biceps and lower trapezius components, suggesting that the lower trapezius may have coactivated during elbow flexion. 
                        Synergy group 3 has strong involvement of the pectoralis major and lower trapezius muscles. These muscles play a role in stabilizing 
                        the shoulder joint. As a result, the muscle activation patterns during this task demonstrates that elbow flexors, shoulder abductors, 
                        and their respective antagonists are recruited in synergistic groups.
                    </p>
                    <h6>Normalized Muscle synergy when shoulder abduction load is 50% MVT Shoulder abduction</h6>
                    <p></p>
                    <img class="img-fluid" src="img/portfolio/muscle_proj/50.png" alt="" width="700" height="300">
                    <p>Figure 9</p>
                    <p></p>
                    <p>There are two muscle synergies decomposed from the sEMG signals collected during the motor task when participants flexed about their 
                        elbow to 25% maximum voluntary elbow flexion and abducted about their shoulder to 50% maximum voluntary shoulder abduction. As shown 
                        in Figure 9, synergy group 1 has a strong biceps component, as well as lower trapezius component, and synergy group 2 has a weak 
                        biceps component and a lower trapezius component. This two-synergy-group result didn't fully align with the initial hypothesis, but 
                        still showed that synergy group 1 mainly explains the elbow flexion and synergy group 2 mainly explains the shoulder abduction.
                    </p>
                   <h5>Comparison of muscle activation patterns between the two tasks
                </h5>
                <p></p>
                    <p>Results of our NMF algorithm demonstrated that four muscle synergies are necessary for reconstructing sEMG signals during the task with a small shoulder abduction load (10% MVT), while only two are needed for the task with a large shoulder abduction load (50% MVT). This result can be supported by the principles underlying muscle recruitment. A motor task requiring high effort and large torque output often involves a gross activation of multiple muscles at once. On the other hand, for a task requiring less net torque output, it is often necessary to use antagonistic muscles to stabilize and maintain the torque output within the desired range. 
                    </p>
                    <h4>Reference</h4>
                    <p><in>[1] Cai NM, Drogos JM, Dewald JPA and Gurari N (2019) Individuals With Hemiparetic Stroke Accurately Match Torques They Generate About Each Elbow Joint. Front. Neurosci. 13:1293. doi: 10.3389/fnins.2019.01293
                    </in></p>
                    <p><in>[2] Lee DD and Seung HS. Learning the parts of objects by non-negative matrix factorization. Nature 401: 788 –791, 1999.
                    </in></p>
                    <p><in>[3] Jinsook Roh, William Z. Rymer, Eric J. Perreault, Seng Bum Yoo, and Randall F. Beer. Alterations in upper limb muscle synergy structure in chronic stroke
                        survivors. J Neurophysiol. 2013 Feb 1; 109(3): 768–781.</in></p>
                    <ul class="list-inline">
                    <p class="item-intro text-muted"> <a class="boxed" href="https://github.com/mushenghe/muscle_synergy_torque_accuracy"><font color="red"><b>Current Status and Source code here</b></font></a></p>
                      <li>Date: December 2019</li>
                     
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    <!-- Healer Baxter -->
    <div class="portfolio-modal modal fade" id="healerbaxter" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Healer Baxter</h3>
                    <a class="boxed">Deep Learning</a>&ensp;<a class="boxed">Tensorflow</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                    <p></p>
                    <h4>Brief Description</h4>       
                  	<p>This project aims to use deep learning and manipulation platform to train a two-armed robot to play the piano based on the human emotion.</p>
                  	<p></p>
                    <h4>Motivation</h4>       
                  	<p>We live in an era in which communication seems simpler than any times, a friend is only one text away or one video chat away. Although communication may be easier and faster, people still feel lonely and depression rates have largely increased. Inspired by this circumstance, this project is to design a system that would enable the Baxter Robot to detect the negative emotion of its master and play songs on the piano using both hands to help him/her get rid of the bad feelings.</p>
                    <p></p>
                    <h4>Emotion Detection</h4>
                    <h5>1. Data Pre-processing</h5> 
                    <p>The datasets are composed of 35887
                      training and testing samples. The training
                      samples are then divided into two sets namely;
                      Training Set and Validation Set. Training set
                      samples composed of 80% of the original dataset
                      samples and 20% of the samples are specified for
                      validation. Hence Training Set and Validation Set
                      will be 22966 and 5741 respectively.
                      Preprocessing is performed to prepare images for
                      the feature extraction stage. A set of facial feature
                      points is extracted from the images then facial
                      features derived from these points. Different sets
                      of facial features are used for both training and
                      validation classifiers.</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/data.png" alt="">
                    <p></p>
                    <h5>2. Data Augmentationg</h5> 
                    <p>In order to avoid overfitting and improve
                      recognition accuracy I applied data
                      augmentation techniques on each training samples. For each image I performed
                      following transforms:
                      a. Rescale (1. / 255)
                      B. Rotation (30)
                      b. Shear (0.3)
                      c. Zoom (0.3)
                      d. shift(width:0.4,height:0.4)
                      e. Flip (horizontal)</p>
                    <p></p>
                    <h5>3. Proposed CNN Model</h5>       
                  	<p>The CNN Model I built takes the input grayscale image size of 48*48 pixels. This model architecture is composed of 5 layers. These layers
                      contains 5 convolutional layers and 5 max pooling layers along with 2 fully connected layer and the output layer. The
                      output layer consists of 5 neurons corresponding to 5
                      emotional labels: Angry, Happy, Neutral, Sad and Surprise. This model uses Rectified Linear Unit (relu) as most precisely used
                      activation function which is applied on all the Convolution
                      Layer and Dense Layer except the last layer (output layer)
                      which is actually Softmax Function. Dropout Layer is also
                      applied after each Convolution, Max Pooling and Dense
                      Layer with the rate of 0.25</p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/CNN_ARC.png" alt="">
                    <p></p>
                  	<h5>Emotion Detection Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/s_xAZkpSfBo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <h4>Piano Playing</h4>
                    <p>The detected emotion would be written in a text file using basic pyhton I/O function. The piano playing node keep reading this text file and when the emotion "sad" is detected and written in the file, baxter would start playing the piano</p>
                    <h5>Locate keys using AprilTags</h5> 
                    <p>Use pose infomation of the two apriltags in left_arm frame to compute the poses of keys in the baxter base frame</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/piano.png" alt="">
                    <h5>Piano Playing Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/lJ78-UlrxD4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/mushenghe/HealerBaxter"><font color="red"><b>Github Page</b> </font></a></p>
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                      
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  
    <!-- Terminator -->
    <div class="portfolio-modal modal fade" id="terminator" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Terminator</h3>
                  <a class="boxed">ROS</a>&ensp;<a class="boxed">Baxter</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                  <p></p>         
                  <h4>Brief Description</h4>       
                  <p>The goal of this project is to enable Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose. </p>
                  <h4>Action Sequence</h4> 
                  <p></p>
                  <p>1. Baxter goes through initial calibration and start up sequence. Arms are moved to an initial pose. <br>
                  2. Baxter finds the nerf gun using an AprilTag and its left arm camera<br>
                  3. Baxter moves its left arm to line up with the nerf gun and closes its gripper to pick up the gun<br>
                  4. Once Baxter has the gun, it uses its left arm camera to find a cup using darknet<br>
                  5. Baxter keeps moving its left arm until the cup is in the center of the image produced by the camera<br>
                  6. Baxter moves its right arm to put its grippers around the nerf gun trigger<br>
                  7. Baxter waits for a user input to confirm the firing of the gun<br>
                  8. Baxter keeps waiting until the user tells it to fire<br>
                  9. Baxter pulls the trigger using its right gripper<br>
                  10. Baxter moves to a final pose right after shooting<br></p>
                  <p></p>
                  <h4>Demo</h4>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/2MRsNefNWmw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p></p>

                  <ul class="list-inline">
                  <p><a class="boxed" href="https://github.com/mushenghe/final-project-terminator"><font color="red"><b>Github Page</font></a></p> </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <!-- KUKA Manipulation -->
    <div class="portfolio-modal modal fade" id="kuka" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Mobile Manipulation</h3>
                  <a class="boxed">Manipulation</a>&ensp;<a class="boxed">Motion Planning</a>&ensp;<a class="boxed">VREP</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to drive the KUKA youBot to pick up a block at the start location, carry it to the desired location, and put it down in the simulation software V-REP. The project covers the following topics: <br>1. Plan a trajectory for the end-effector of the youBot mobile manipulator. <br>2. Generate the kinematics model of the youBot, consisting of the mobile base with 4 mecanum wheels and the robot arm with 5 joints<br>3. Apply feedback control to drive the robot to implement the desired task<br>4. Conduct the simulations in V-REP</p>                           
                  <h5>First Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/C3QQO7TZn4g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <h5>Second Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/1WKscbUi3HA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p><a class="boxed" href="https://github.com/mushenghe/Mobile-Manipulation-"><font color="red"><b>Github Page</font></a></p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Visual Pushing and Grasping -->
    <div class="portfolio-modal modal fade" id="pushinggrasping" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                <div class="rl"></div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Visual Pushing and Grasping</h3>
                    <a class="boxed">Deep Reinforcement Learning</a>&ensp;<a class="boxed">Docker</a>&ensp;<a class="boxed">Pytorch</a>
                    <p></p>
                    <h4>Brief Description</h4> 
                    <p>Most grasping algorithms today often fail to handle scenarios where objects are tightly packed together. They can attempt bad grasps repeatedly to no avail since they can only find accessible grasps. This project proposed to discover and learn synergies between pushing and grasping from experience through model-free deep reinforcement learning.</p>
                    <h5>System Overview</h5> 
                    <p></p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/pushing_grasping/method.jpg" alt="">
                    <p></p>
                    <h5>Model Input & Output</h5>
                    <p></p>
                    <p>The Q-function is modeled as two feed-forward fully convolutional networks(FCNs) Φp and Φg. FCN Φp is for pushing motion primitive behavior and FCN Φg is for grasping.</p>
                    <p></p>
                    <p>For each individual FCN Φψ:</p>
                    <p></p>
                    <p>Input: the heightmap image representation of the current state</p>
                    <p>Output: a dense pixel-wisemap of Q values with the same image size and resolution as that of the state</p>
                    <p></p>
                    <p>Note: each individual Q value prediction at a pixel p represents the future expected reward of executing primitive ψ at 3D location q where q→p ∈st.</p>  
                    
                    <p></p>
                    <h5>Use the pretrained model</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/i_fHtQ-zDkM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/mushenghe/visual-pushing-grasping"><font color="red"><b>Github Page</font></a></p> 
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                        <i class="fas fa-times"></i>
                        Close Project</button>
                    </div>
                </div>
                </div>
            </div>
            </div>
        </div>
        </div>
      

    <!-- Panorama -->
    <div class="portfolio-modal modal fade" id="panorama" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">ImageMosaic</h3>
                  <a class="boxed">Computer Vision</a>&ensp;<a class="boxed">MATLAB</a>&ensp;<a class="boxed">Python</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to Create an image panorama by stitching a set of images together</p>                           
                  <h5>Image Registration</h5>
                  <p>I used SURF to do the feature point extraction and matching, then used random sample consensus(RANSAC) for transform matrix estimation</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/SURFMatches.jpg" alt="">
                  <h5>Image Warping</h5>
                  <p>Use the derived transform matrix nad project that warped image on a plain surface</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img2.jpg" alt="">
                  <h5>Image Blending</h5>
                  <p>Using Center-Weighting algorithm (compute the the distance from each pixel to 4 boundaries of the image and take the the smallest ratio                   
                    between two distances and the dimension of image as the corresponding pixel
                    value on mask matrix). The mask we derived is shown in the following image:</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/mask.jpg" alt="">
                  <p>For each image, I derive a mask and then warp the mask just as warp the image</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/before.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/after.jpg" alt="">
                  <h5>Cropping</h5>
                  <p>After doing image stitching and image blending, I get the panorama look as following</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3.jpg" alt="">
                  <p>Use pythong to find the largest rectangle that don’t include the black region in the
                    panorama image, I get the final panorama look as following</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3_a.jpg" alt="">
                  <ul class="list-inline">
                  <p><a class="boxed" href="https://drive.google.com/file/d/1p8h66kcTu3J3DAQ-OETgzaHgj101Kw8o/view?usp=sharing"><font color="red"><b>Report</font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Dynamics -->
    <div class="portfolio-modal modal fade" id="dynamics" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Pin-triangle Dynamics Simulation </h3>
                  <a class="boxed">Python</a>&ensp;<a class="boxed">Dynamics</a>&ensp;<a class="boxed">Simulation</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>This project is a dynamics simulation of a triangle bouncing in the enclosed rectangle. This project shows techniques expansion from theory and ability of building physical model.  </p>
                  <p></p>
                  <h5>dynamic model</h5>
                  <p>The pictured pin-triangle is a constrained system involving 2 bodies: a equilateral triangle and a square. the triangle has length d = 3, mass m =0.5 and rotational inertia J=1 (assuming that the center of mass is at the center of geometry). The square has length D = 20, mass M = 5 and rotational inertia J=3 (assuming that the center of mass is at the center of geometry). The triangle has configuration (x,y,\theta_t) and the square has an angle \theta_s relative to the world frame. The triangle is constrainted to not bounce out of the square.</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/dynamics_proj/model.png" alt="">
                  <ul class="list-inline">
                 	<p><a class="boxed" href="https://colab.research.google.com/drive/1d3GrjH75j-MqSYNUyhwd3FWeb2eVgY4D"><font color="red"><b>Python Source Code</font></a></p>
                    
                   
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                    
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

   

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
