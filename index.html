<!DOCTYPE html>
<html lang="en">

  <head>
    <title> Musheng He</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <!--<title>Agency - Start Bootstrap Theme</title>-->

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Musheng He</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">           
          <div class="intro-heading text-uppercase"style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Musheng He</div>
          <div class="intro-lead-in">Deep Reinforcement Learning for Human-Robot Interaction</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/1waCBEbzmcRop_moeXQEUnuNYvaxCvS1X/view?usp=sharing">Resume</a>
        </div>
      </div>
    </header>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <h3 class="section-subheading text-muted">September 2019 ~ Now</h3>
          </div>
        </div>
        <div class="row">

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal1">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/mind_proj/nerfgun.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Terminator</h4>
              <p class="text-muted">ROS | CV | Motion Planning<br>Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
              <a class="portfolio-link" data-toggle="modal" href="#portfolio_eeme">
                <div class="portfolio-hover">
                  <div class="portfolio-hover-content">
                    <i class="fas fa-plus fa-3x"></i>
                  </div>
                </div>
                <img class="img-fluid" src="img/portfolio/healer_baxter/healer.gif" alt="" width="400" height="300">
              </a>
              <div class="portfolio-caption">
                <h4>Healer Baxter</h4>
                <p class="text-muted">Deep Learning | CV | Motion Planning<br>Baxter Robot play the piano based on detected human emotion</p>
              </div>
            </div>

            <div class="col-md-4 col-sm-6 portfolio-item">
                <a class="portfolio-link" data-toggle="modal" href="#portfolioModal3">
                  <div class="portfolio-hover">
                    <div class="portfolio-hover-content">
                      <i class="fas fa-plus fa-3x"></i>
                    </div>
                  </div>
                  <img class="img-fluid" src="img/portfolio/manipulation_proj/manipulation.gif" alt="" width="400" height="300">
                </a>
                <div class="portfolio-caption">
                  <h4>Mobile Manipulation</h4>
                  <p class="text-muted">Manipulation | Motion Planning<br>KUKA youBot to pick up a block at the start location and carry it to the desired location in the simulation software V-REP</p>
                </div>
              </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal2">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/pushing_grasping/rep.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Visual Pushing and Grasping</h4>
              <p class="text-muted">Deep Reinforcement Learning | Docker <br>Train robotic agents to learn to plan pushing and grasping actions for manipulation with deep reinforcement learning</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal4">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/image_mosaic/panorama.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>ImageMosaic</h4>
              <p class="text-muted">Computer Vision<br>Create an image panorama by stitching a set
                of images together</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal5">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/dynamics_proj/314project.gif" alt="" width="400" height="150">
            </a>
            <div class="portfolio-caption">
              <h4>Pin-triangle Dynamics Simulation </h4>
              <p class="text-muted">Dynamics | Simulation<br>Simulation of a triangle bouncing in the enclosed rectangle</p>
            </div>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Technical Skills -->
    <section  id="TechnicalSkills">
        <div class="container">
          <div class="row">
            <div class="col-lg-12 text-center">
              <h2 class="section-heading text-uppercase">Skills</h2>
              <p></p>
            </div>
          </div>
          <div class="row text-center">
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <!-- <i class="fas fa-circle fa-stack-2x text-primary"></i> -->
                <img class="mx-auto rounded-circle" src="img/skills/electronicdesign.png" alt="" width="120" height="120">
              </span>
              <h4 class="service-heading">Electrical Engineering</h4>
              <p class="text-muted">Power System, Microprocessor<br>Circuit Design, Electronic System Design</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/coding.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Programming</h4>
              <p class="text-muted">C，C++, Python, Java, MATLAB, PLC, VHDL</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/manufacturing.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Manufacturing</h4>
              <p class="text-muted"> Laser Cutter, 3D print, soldering </p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/microcontroller.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Robotics</h4>
              <p class="text-muted"> ROS, Linux, Git <br> Computer Vision, Machine Learning<br> Motion Planning，Feedback Control，Deep Learning</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/cad.png" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Mechanical Engineering</h4>
              <p class="text-muted">Onshape, AutoCAD<br> Mechatronics/ Microcontroller</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/machine_learning.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Research Interest</h4>
              <p class="text-muted">Deep Reinforcement Learning<br> Manipulation</p>
            </div>
          </div>
        </div>
      </section>
    
    <!-- contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
            <div class="col-lg-12 max-auto mb-5">
                <img class="mx-auto d-block rounded-circle" src="img/profilepic.jpg" alt="Avatar" width="300" height="300">
            </div>
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Contact me</h2>
           
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
         

            <div class="team-member">
              
              <h4>Musheng He</h4>
              <p class="text-muted">Master of Science in Robotics @ Northwestern University</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/mushenghe">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:mushenghe2020@u.northwestern.edu">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/musheng-he-a50277176/">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-8 mx-auto text-center">
           
          </div>
        </div>
      </div>
    </section>


    <!-- Portfolio Modals -->

    <!-- Modal stroke -->
    <div class="portfolio-modal modal fade" id="portfolio_stroke" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">A Mechatronic System to Study Proprioception (Ongoing Project) </h3>
                    <a class="boxed">HCI</a>&ensp;<a class="boxed">Biosensor</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Interfacing</a>&ensp;<a class="boxed">Mechanical Design</a>&ensp;
                    <p></p>
                    <iframe width="700" height="467" src="https://www.youtube.com/embed/ntOk0ySuN0E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <p></p>
                    <p><br>The objective of this project is to build a mechatronic system to study proprioception in young children. This system includes a mechanical design for the experimental setup, and software for acquiring sensor data and delivering automated audiovisual feedback. The targeted age of children is between 3 and 6 years old. The developed system provides visually attractive interfaces that adapt to the participant’s interactions. The experiment contains a test to quantify a participant’s maximum voluntary torque, as well as their perception. This system runs an automated experiment and records data for post-hoc analyses.
                    </p>
                    <h4>Motivation</h4>
                    
                    <p>Numerous studies have quantified proprioception in adults. However, less information is available about children. The goal of this project is to develop an experiment to permit proprioception to be quantified in children. The long-term goal of this project is to, using the system developed, understand the extent to which children who have neurological impairments also have proprioceptive deficits.</p>
                    <p></p>
                    <h4>Method</h4>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/stroke_proj/expsetup.jpg" alt="">
                    <p><i>Experiment Setup</i></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/stroke_proj/visual_pipeline.png" alt="">
                    <p><i>The system acquires torque data indicating the extent to which the participant is flexing and extending about each elbow joint through two torque sensors. We used a DAQ card for data acquisition. A python script streams the sensory data at 1000Hz and passes signals to a callback function. The callback function utilizes the linear equation resulted from the [calibration](calibration/README.md) process to convert a voltage signal to the actual torque generated by the participant. A zeroing function can also be called to remove an offset in the voltage signal. The perceptual testing tasks use the results from the maximum voluntary torque test (maximum torque), as well as the real-time torque to determine the audiovisual feedback. The system renders the visual interface at 27Hz and records data at 1000Hz. </i></p>
                    
                    <ul class="list-inline">
                    <p class="item-intro text-muted"> <a class="boxed" href="https://github.com/monkalynn813/children_ability_assessment_sys"><font color="red"><b>Current Status and Source code here</b></font></a></p>
                      <li>Date: December 2019</li>
                     
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    <!-- Modal electromechanical -->
    <div class="portfolio-modal modal fade" id="portfolio_eeme" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Healer Baxter</h3>
                    <a class="boxed">Deep Learning</a>&ensp;<a class="boxed">Tensorflowr</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                    <p></p>
                    <h4>Brief Description</h4>       
                  	<p>This project aims to use deep learning and manipulation platform to train a two-armed robot to play the piano based on the human emotion.</p>
                  	<p></p>
                    <h4>Motivation</h4>       
                  	<p>We live in an era in which communication seems simpler than any times, a friend is only one text away or one video chat away. Although communication may be easier and faster, people still feel lonely and depression rates have largely increased. Inspired by this circumstance, this project is to design a system that would enable the Baxter Robot to detect the negative emotion of its master and play songs on the piano using both hands to help him/her get rid of the bad feelings.</p>
                    <p></p>
                    <h4>Emotion Detection</h4>
                    <h5>1. Data Pre-processing</h5> 
                    <p>The datasets are composed of 35887
                      training and testing samples. The training
                      samples are then divided into two sets namely;
                      Training Set and Validation Set. Training set
                      samples composed of 80% of the original dataset
                      samples and 20% of the samples are specified for
                      validation. Hence Training Set and Validation Set
                      will be 22966 and 5741 respectively.
                      Preprocessing is performed to prepare images for
                      the feature extraction stage. A set of facial feature
                      points is extracted from the images then facial
                      features derived from these points. Different sets
                      of facial features are used for both training and
                      validation classifiers.</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/data.png" alt="">
                    <p></p>
                    <h5>2. Data Augmentationg</h5> 
                    <p>In order to avoid overfitting and improve
                      recognition accuracy I applied data
                      augmentation techniques on each training samples. For each image I performed
                      following transforms:
                      a. Rescale (1. / 255)
                      B. Rotation (30)
                      b. Shear (0.3)
                      c. Zoom (0.3)
                      d. shift(width:0.4,height:0.4)
                      e. Flip (horizontal)</p>
                    <p></p>
                    <h5>3. Proposed CNN Model</h5>       
                  	<p>The CNN Model I built takes the input grayscale image size of 48*48 pixels. This model architecture is composed of 5 layers. These layers
                      contains 5 convolutional layers and 5 max pooling layers along with 2 fully connected layer and the output layer. The
                      output layer consists of 5 neurons corresponding to 5
                      emotional labels: Angry, Happy, Neutral, Sad and Surprise. This model uses Rectified Linear Unit (relu) as most precisely used
                      activation function which is applied on all the Convolution
                      Layer and Dense Layer except the last layer (output layer)
                      which is actually Softmax Function. Dropout Layer is also
                      applied after each Convolution, Max Pooling and Dense
                      Layer with the rate of 0.25</p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/CNN_ARC.png" alt="">
                    <p></p>
                  	<h5>Emotion Detection Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/s_xAZkpSfBo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <h4>Piano Playing</h4>
                    <p>The detected emotion would be written in a text file using basic pyhton I/O function. The piano playing node keep reading this text file and when the emotion "sad" is detected and written in the file, baxter would start playing the piano</p>
                    <h5>Locate keys using AprilTags</h5> 
                    <p>Use pose infomation of the two apriltags in left_arm frame to compute the poses of keys in the baxter base frame</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/healer_baxter/piano.png" alt="">
                    <h5>Piano Playing Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/lJ78-UlrxD4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/mushenghe/HealerBaxter"><font color="red"><b>Github Page</b> </font></a></p>
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                      
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  
    <!-- Modal mindwave -->
    <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Terminator</h3>
                  <a class="boxed">ROS</a>&ensp;<a class="boxed">Baxter</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">Motion Planning</a>
                  <p></p>         
                  <h4>Brief Description</h4>       
                  <p>The goal of this project is to enable Baxter Robot pick up a nerf gun, locate a cup, pull the nerf gun trigger to shoot the cup when given a user input, and move to a final pose. </p>
                  <h4>Action Sequence</h4> 
                  <p></p>
                  <h5>Baxter goes through initial calibration and start up sequence. Arms are moved to an initial pose.</h5>
                  <h5>Baxter finds the nerf gun using an AprilTag and its left arm camera</h5>
                  <h5>Baxter moves its left arm to line up with the nerf gun and closes its gripper to pick up the gun</h5>
                  <h5>Once Baxter has the gun, it uses its left arm camera to find a cup using darknet</h5>
                  <h5>Baxter keeps moving its left arm until the cup is in the center of the image produced by the camera</h5>
                  <h5>Baxter moves its right arm to put its grippers around the nerf gun trigger</h5>
                  <h5>Baxter waits for a user input to confirm the firing of the gun</h5>
                  <h5>Baxter keeps waiting until the user tells it to fire</h5>
                  <h5>Baxter pulls the trigger using its right gripper</h5>
                  <h5>Baxter moves to a final pose right after shooting</h5>
                  <p></p>
                  <h4>Demo</h4>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/2MRsNefNWmw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p></p>

                  <ul class="list-inline">
                  <p><a class="boxed" href="https://github.com/mushenghe/final-project-terminator"><font color="red"><b>Github Page</font></a></p> </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2 -->
    <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Visual Pushing and Grasping</h3>
                  <a class="boxed">Deep Reinforcement Learning</a>&ensp;<a class="boxed">Docker</a>&ensp;<a class="boxed">Pytorch</a>
                  <p></p>
                  <h4>Brief Description</h4> 
                  <p>Most grasping algorithms today often fail to handle scenarios where objects are tightly packed together. They can attempt bad grasps repeatedly to no avail since they can only find accessible grasps. This project proposed to discover and learn synergies between pushing and grasping from experience through model-free deep reinforcement learning.</p>
                  <h5>System Overview</h5> 
                  <p></p>
                  <p></p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/pushing_grasping/method.jpg" alt="">
                  <p></p>
                  <h5>Model Input & Output</h5>
                  <p></p>
                  <p>The Q-function is modeled as two feed-forward fully convolutional networks(FCNs) Φp and Φg. FCN Φp is for pushing motion primitive behavior and FCN Φg is for grasping.</p>
                  <p></p>
                  <p>For each individual FCN Φψ:</p>
                  <p></p>
                  <p>Input: the heightmap image representation of the current state</p>
                  <p>Output: a dense pixel-wisemap of Q values with the same image size and resolution as that of the state</p>
                  <p></p>
                  <p>Note: each individual Q value prediction at a pixel p represents the future expected reward of executing primitive ψ at 3D location q where q→p ∈st.</p>  
                  
                  <p></p>
                  <h5>Use the pretrained model</h5> 
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/i_fHtQ-zDkM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <ul class="list-inline">
                  <p><a class="boxed" href="https://github.com/mushenghe/visual-pushing-grasping"><font color="red"><b>Github Page</font></a></p> 
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3 -->
    <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Mobile Manipulation</h3>
                  <a class="boxed">Manipulation</a>&ensp;<a class="boxed">Motion Planning</a>&ensp;<a class="boxed">VREP</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to drive the KUKA youBot to pick up a block at the start location, carry it to the desired location, and put it down in the simulation software V-REP. The project covers the following topics: <br>1. Plan a trajectory for the end-effector of the youBot mobile manipulator. <br>2. Generate the kinematics model of the youBot, consisting of the mobile base with 4 mecanum wheels and the robot arm with 5 joints<br>3. Apply feedback control to drive the robot to implement the desired task<br>4. Conduct the simulations in V-REP</p>                           
                  <h5>First Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/C3QQO7TZn4g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <h5>Second Task Demo</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/1WKscbUi3HA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p><a class="boxed" href="https://github.com/mushenghe/Mobile-Manipulation-"><font color="red"><b>Github Page</font></a></p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 4 -->
    <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">ImageMosaic</h3>
                  <a class="boxed">Computer Vision</a>&ensp;<a class="boxed">MATLAB</a>&ensp;<a class="boxed">Python</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to Create an image panorama by stitching a set of images together</p>                           
                  <h5>Image Registration</h5>
                  <p>I used SURF to do the feature point extraction and matching, then used random sample consensus(RANSAC) for transform matrix estimation</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/SURFMatches.jpg" alt="">
                  <h5>Image Warping</h5>
                  <p>Use the derived transform matrix nad project that warped image on a plain surface</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img2.jpg" alt="">
                  <h5>Image Blending</h5>
                  <p>Using Center-Weighting algorithm (compute the the distance from each pixel to 4 boundaries of the image and take the the smallest ratio                   
                    between two distances and the dimension of image as the corresponding pixel
                    value on mask matrix). The mask we derived is shown in the following image:</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/mask.jpg" alt="">
                  <p>For each image, I derive a mask and then warp the mask just as warp the image</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/before.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/after.jpg" alt="">
                  <h5>Cropping</h5>
                  <p>After doing image stitching and image blending, I get the panorama look as following</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3.jpg" alt="">
                  <p>Use pythong to find the largest rectangle that don’t include the black region in the
                    panorama image, I get the final panorama look as following</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/1_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/2_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/3_a.jpg" alt="">
                  <ul class="list-inline">
                  <p><a class="boxed" href="https://drive.google.com/file/d/1p8h66kcTu3J3DAQ-OETgzaHgj101Kw8o/view?usp=sharing"><font color="red"><b>Report</font></a></p>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 5 -->
    <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Pin-triangle Dynamics Simulation </h3>
                  <a class="boxed">Python</a>&ensp;<a class="boxed">Dynamics</a>&ensp;<a class="boxed">Simulation</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>This project is a dynamics simulation of a triangle bouncing in the enclosed rectangle. This project shows techniques expansion from theory and ability of building physical model.  </p>
                  <p></p>
                  <h5>dynamic model</h5>
                  <p>The pictured pin-triangle is a constrained system involving 2 bodies: a equilateral triangle and a square. the triangle has length d = 3, mass m =0.5 and rotational inertia J=1 (assuming that the center of mass is at the center of geometry). The square has length D = 20, mass M = 5 and rotational inertia J=3 (assuming that the center of mass is at the center of geometry). The triangle has configuration (x,y,\theta_t) and the square has an angle \theta_s relative to the world frame. The triangle is constrainted to not bounce out of the square.</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/dynamics_proj/model.png" alt="">
                  <ul class="list-inline">
                 	<p><a class="boxed" href="https://colab.research.google.com/drive/1d3GrjH75j-MqSYNUyhwd3FWeb2eVgY4D"><font color="red"><b>Python Source Code</font></a></p>
                    
                   
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                    
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 6 -->
    <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Dermatology Diagnose with Machine Learning</h3>
                  <a class="boxed">Python</a>&ensp;<a class="boxed">Machine Learning</a>&ensp;<a class="boxed">Disease Prediction</a><p></p>
                  <iframe width="700" height="467" src="https://www.youtube.com/embed/_uNckn339hY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  <p>The project is to perform dermatology diagnose through machine learning techniques those built from scratch. The algorithms used in this project includes feature selection, boosting selection, gradient descent, and fusion rule. The GUI was allowed to return the probability of certain prediction back to user. More detail was explained through the above demonstration video.</p>
                  <ul class="list-inline">
                      <p class="item-intro text-muted"> one can find <a class="boxed" href="https://github.com/monkalynn813/ml_disease_diag"><font color="red"><b>Python source code</b> here</font></a></p>
                    <li>Date: November-2018</li>
          
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
